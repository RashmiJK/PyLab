{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbafd4f9",
   "metadata": {},
   "source": [
    "## Asyncio\n",
    "\n",
    "**You can only use await inside of functions created with async def**\n",
    "\n",
    "Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95e784",
   "metadata": {},
   "source": [
    "### Difference between Threads and Coroutines\n",
    "Threads and coroutines in Python are both mechanisms for achieving concurrency, but they are fundamentally different in how they operate and are managed.\n",
    "\n",
    "**Threads** are managed by the operating system. Each thread represents a separate flow of execution, with its own stack and memory space. Threads can run in parallel (especially on multi-core CPUs), and the OS schedules when each thread runs. This makes threads suitable for tasks that require true parallelism, such as CPU-bound operations. However, threads come with overhead due to context switching and require careful synchronization when accessing shared resources. who understands this better than an embedded systems engineer.\n",
    "\n",
    "**Coroutines** are a programming construct managed at the language level (not by the OS). In Python, coroutines are special functions that can pause (yield) their execution and resume later, allowing other coroutines to run in the meantime. This is known as cooperative multitasking: only one coroutine runs at a time within a **single thread**, and they explicitly yield control to each other. Coroutines are lightweight, have much lower overhead than threads, and are especially well-suited for I/O-bound scenarios. Coroutines are used to perform multiple tasks cooperatively within a single thread, allowing more efficient use of resources and easier management of concurrent tasks. \n",
    "\n",
    "| Feature         | Threads                                 | Coroutines                                   |\n",
    "|-----------------|-----------------------------------------|-----------------------------------------------|\n",
    "| **Management**  | OS-level                               | Language/runtime-level                        |\n",
    "| **Parallelism** | Can run in parallel (multi-core CPUs)  | Run cooperatively within a single thread      |\n",
    "| **Overhead**    | High (context switching, memory)       | Low (lightweight, minimal context switching)  |\n",
    "| **Use case**    | CPU-bound, true parallelism            | I/O-bound, high concurrency, async tasks      |\n",
    "| **Control**     | Preemptive (OS decides when to switch) | Cooperative (programmer decides when to yield)|\n",
    "\n",
    "\n",
    "In summary, threads are for parallel execution managed by the OS, while coroutines are for cooperative multitasking managed by Python itself. Coroutines are not a replacement for threads, but offer a more efficient and simpler way to handle many concurrent I/O-bound tasks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cc47b",
   "metadata": {},
   "source": [
    "### Concurrency and Parallelism: Key Concepts\n",
    "\n",
    "What Is Concurrency?\n",
    "Concurrency in computing means the ability of a system to handle multiple tasks or operations at (almost) the same time. These tasks may not actually run simultaneously—instead, they are managed so that progress is made on each over a period of time. The operating system or language runtime switches between them efficiently, giving the illusion that they are happening together.\n",
    "    * Concurrency is about dealing with many things at once, not necessarily doing them at the same instant.\n",
    "    * Examples: Handling multiple network requests in a web server, or responsive user interfaces where several activities seem active together.\n",
    "\n",
    "What Is Parallelism?\n",
    "Parallelism refers to actually performing multiple computations or tasks at the same time. This is only possible on hardware with multiple processors or cores.\n",
    "    * Parallelism is about doing many things simultaneously, leveraging the hardware’s ability to run code on different cores or machines.\n",
    "    * Examples: Processing parts of a large image on different CPU cores, or running multiple calculations at the same time in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323954c",
   "metadata": {},
   "source": [
    "How Do Threads and Coroutines Relate?\n",
    "Threads\n",
    "\t•\tThreads are a way to achieve both concurrency and, on multi-core systems, parallelism.\n",
    "\t•\tIn Python, threads allow multiple parts of a program to appear to run independently. Operating systems can truly run threads in parallel on different CPU cores.\n",
    "\t•\tHowever, due to Python’s Global Interpreter Lock (GIL), true parallelism is limited for CPU-bound tasks in standard CPython. For I/O-bound programs (like waiting for files or network), threads are very effective.\n",
    "\n",
    "| Feature                | Threads                                   |\n",
    "|------------------------|-------------------------------------------|\n",
    "| Managed by             | Operating system                          |\n",
    "| Can provide parallelism| Yes, on multi-core CPUs                   |\n",
    "| Use-case               | Both concurrency and parallelism          |\n",
    "| Typical tasks          | I/O- or CPU-bound                         |\n",
    "\n",
    "\n",
    "Coroutines\n",
    "\t•\tCoroutines are a lightweight form of concurrency, managed by Python itself (not the OS).\n",
    "\t•\tThey enable the program to switch between tasks at certain points (like when one is waiting for data), but only one coroutine runs at a time within a single thread.\n",
    "\t•\tCoroutines do not give true parallelism (all run on the same thread) but can provide very high concurrency for I/O-bound programs, such as thousands of simultaneous web requests.\n",
    "\n",
    "| Feature                | Coroutines                                |\n",
    "|------------------------|-------------------------------------------|\n",
    "| Managed by             | Python interpreter (language-level)       |\n",
    "| Can provide parallelism| No (single thread, cooperative switching) |\n",
    "| Use-case               | High concurrency, I/O-bound workloads     |\n",
    "| Typical tasks          | Asynchronous network/server programming   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f18dc",
   "metadata": {},
   "source": [
    "#### Comparing Concurrency and Parallelism in Python\n",
    "\n",
    "| Mechanism    | Concurrency | Parallelism | Where Used       | When to Use                                   |\n",
    "|--------------|-------------|-------------|------------------|-----------------------------------------------|\n",
    "| Threads      | Yes         | Limited*    | OS-level tasks   | I/O-bound (good), CPU-bound (limited by GIL)  |\n",
    "| Coroutines   | Yes         | No          | async/await code | High I/O concurrency, very lightweight        |\n",
    "\n",
    "*Python threads can use multiple cores only in implementations without the GIL, or via multiprocessing modules.\n",
    "Summary\n",
    "\t•\tConcurrency is about managing many tasks efficiently, switching between them.\n",
    "\t•\tParallelism is about running multiple tasks at the same time, on separate processors/cores.\n",
    "\t•\tThreads can give both, but Python’s GIL restricts true CPU-based parallelism (except for I/O tasks).\n",
    "\t•\tCoroutines offer high concurrency by pausing and resuming tasks in the same thread, with minimal resource use, but do not provide actual parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7dfa4",
   "metadata": {},
   "source": [
    "`asyncio` module cannot be used in Python environments that run on WebAssembly platforms. \n",
    "\n",
    "The main reason is that these platforms do not support many of the low-level system features required by `asyncio`, such as system calls and event loop support. \n",
    "\n",
    "Therefore, try running the examples in this guide in a regular terminal environment where all necessary system features are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587a1cb",
   "metadata": {},
   "source": [
    "## Streaming in OpenAI API\n",
    "Streaming makes large language model (LLM) apps feel faster and more interactive. Instead of waiting for the full response to be generated, the API sends it in small parts — token by token — as soon as they’re ready. This improves user experience, reduces memory load, allows early cancellation, and handles long responses more efficiently.\n",
    "\n",
    "In OpenAI's API, this is enabled by setting stream=True in the request. When streaming is enabled, the response is delivered using **Server-Sent Events (SSE)** over HTTP with chunked transfer encoding. [Technically, stream=True enables server-sent events (SSE), so the HTTP response is kept open and data is sent incrementally using chunked transfer encoding]\n",
    "\n",
    "Below, we’ll look at how responses differ with and without streaming, then explore how tools like LangChain and LangGraph build on this concept using Python's async and await."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0db9ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00df7f",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **without** Streaming\n",
    "This code invokes the OpenAI API using the synchronous Python client and does not use streaming. The entire model output is returned in one HTTP response, and the call is blocking: program execution pauses until the response arrives. This approach is straightforward and suitable for scripts or applications where concurrent handling of multiple requests or real-time partial output is not required.\n",
    "* “Synchronous” means the program waits for the API call to complete before moving forward.\n",
    "* “Blocking” means no other statements after the API call are executed until the full response is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61ce177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(response): <class 'openai.types.chat.chat_completion.ChatCompletion'> \n",
      "\n",
      "Sure! Here it is:\n",
      "\n",
      "double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"typeof(response):\", type(response), \"\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5304d",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **with** Streaming\n",
    "When `stream=True` is set, the OpenAI API keeps the HTTP connection open and transmits the model’s output incrementally as it is generated. This allows your program to receive and process each chunk of the response in real time, rather than waiting for the complete output. Internally, the API sends these chunks using server-sent events and chunked transfer encoding, enabling your application to start displaying or utilizing the model’s reply as soon as the first data arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb68854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(stream): <class 'openai.Stream'>  Is Generator:  False  Is Iterator:  True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This script demonstrates how to use the OpenAI Python client to create a response with stream=True\n",
    "\n",
    "from openai import OpenAI\n",
    "from types import GeneratorType\n",
    "from collections.abc import Iterator\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print(\"typeof(stream):\", type(stream), \" Is Generator: \", isinstance(stream, GeneratorType), \" Is Iterator: \", isinstance(stream, Iterator), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "767e99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95d76343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.None"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac16880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for event in stream:\n",
    "\n",
    "    print(\"****\" * 20)\n",
    "    print(json.dumps(event.model_dump(), indent=2))\n",
    "    print(\"@@@@\" * 20)\n",
    "    print(event)\n",
    "    print(\"----\" * 20)\n",
    "\n",
    "    print(event.choices[0].delta.content, end=\"\", flush=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624115ac",
   "metadata": {},
   "source": [
    "### ASynchronous OpenAI API Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"<ur key>\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for event in stream:\n",
    "        print(event)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3ae72",
   "metadata": {},
   "source": [
    "### Generator 'yiled' and Generator expression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
