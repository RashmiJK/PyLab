{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbafd4f9",
   "metadata": {},
   "source": [
    "## Asyncio\n",
    "\n",
    "You can only use await inside of functions created with async def.\n",
    "\n",
    "Concurrency\n",
    "Parallelism\n",
    "\n",
    "\n",
    "\n",
    "Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95e784",
   "metadata": {},
   "source": [
    "### Difference between Threads and Coroutines\n",
    "Threads and coroutines in Python are both mechanisms for achieving concurrency, but they operate fundamentally different in how they operate and are managed.\n",
    "\n",
    "**Threads** are managed by the operating system. Each thread represents a separate flow of execution, with its own stack and memory space. Threads can run in parallel (especially on multi-core CPUs), and the OS schedules when each thread runs. This makes threads suitable for tasks that require true parallelism, such as CPU-bound operations. However, threads come with overhead due to context switching and require careful synchronization when accessing shared resources. who understands this better than an embedded systems engineer.\n",
    "\n",
    "**Coroutines** are a programming construct managed at the language level (not by the OS). In Python, coroutines are special functions that can pause (yield) their execution and resume later, allowing other coroutines to run in the meantime. This is known as cooperative multitasking: only one coroutine runs at a time within a **single thread**, and they explicitly yield control to each other. Coroutines are lightweight, have much lower overhead than threads, and are especially well-suited for I/O-bound scenarios. Coroutines are used to perform multiple tasks cooperatively within a single thread, allowing more efficient use of resources and easier management of concurrent tasks. \n",
    "\n",
    "| Feature         | Threads                                 | Coroutines                                   |\n",
    "|-----------------|-----------------------------------------|-----------------------------------------------|\n",
    "| **Management**  | OS-level                               | Language/runtime-level                        |\n",
    "| **Parallelism** | Can run in parallel (multi-core CPUs)  | Run cooperatively within a single thread      |\n",
    "| **Overhead**    | High (context switching, memory)       | Low (lightweight, minimal context switching)  |\n",
    "| **Use case**    | CPU-bound, true parallelism            | I/O-bound, high concurrency, async tasks      |\n",
    "| **Control**     | Preemptive (OS decides when to switch) | Cooperative (programmer decides when to yield)|\n",
    "\n",
    "\n",
    "In summary, threads are for parallel execution managed by the OS, while coroutines are for cooperative multitasking managed by Python itself. Coroutines are not a replacement for threads, but offer a more efficient and simpler way to handle many concurrent I/O-bound tasks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587a1cb",
   "metadata": {},
   "source": [
    "## Streaming in OpenAI API\n",
    "Streaming makes large language model (LLM) apps feel faster and more interactive. Instead of waiting for the full response to be generated, the API sends it in small parts — token by token — as soon as they’re ready. This improves user experience, reduces memory load, allows early cancellation, and handles long responses more efficiently.\n",
    "\n",
    "In OpenAI's API, this is enabled by setting stream=True in the request. When streaming is enabled, the response is delivered using **Server-Sent Events (SSE)** over HTTP with chunked transfer encoding. [Technically, stream=True enables server-sent events (SSE), so the HTTP response is kept open and data is sent incrementally using chunked transfer encoding]\n",
    "\n",
    "Below, we’ll look at how responses differ with and without streaming, then explore how tools like LangChain and LangGraph build on this concept using Python's async and await."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0db9ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00df7f",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **without** Streaming\n",
    "This code invokes the OpenAI API using the synchronous Python client and does not use streaming. The entire model output is returned in one HTTP response, and the call is blocking: program execution pauses until the response arrives. This approach is straightforward and suitable for scripts or applications where concurrent handling of multiple requests or real-time partial output is not required.\n",
    "* “Synchronous” means the program waits for the API call to complete before moving forward.\n",
    "* “Blocking” means no other statements after the API call are executed until the full response is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61ce177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(response): <class 'openai.types.chat.chat_completion.ChatCompletion'> \n",
      "\n",
      "Sure! Here it is:\n",
      "\n",
      "double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"typeof(response):\", type(response), \"\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5304d",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **with** Streaming\n",
    "When `stream=True` is set, the OpenAI API keeps the HTTP connection open and transmits the model’s output incrementally as it is generated. This allows your program to receive and process each chunk of the response in real time, rather than waiting for the complete output. Internally, the API sends these chunks using server-sent events and chunked transfer encoding, enabling your application to start displaying or utilizing the model’s reply as soon as the first data arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb68854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(stream): <class 'openai.Stream'>  Is Generator:  False  Is Iterator:  True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This script demonstrates how to use the OpenAI Python client to create a response with stream=True\n",
    "\n",
    "from openai import OpenAI\n",
    "from types import GeneratorType\n",
    "from collections.abc import Iterator\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print(\"typeof(stream):\", type(stream), \" Is Generator: \", isinstance(stream, GeneratorType), \" Is Iterator: \", isinstance(stream, Iterator), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "767e99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95d76343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.None"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac16880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for event in stream:\n",
    "\n",
    "    print(\"****\" * 20)\n",
    "    print(json.dumps(event.model_dump(), indent=2))\n",
    "    print(\"@@@@\" * 20)\n",
    "    print(event)\n",
    "    print(\"----\" * 20)\n",
    "\n",
    "    print(event.choices[0].delta.content, end=\"\", flush=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624115ac",
   "metadata": {},
   "source": [
    "### ASynchronous OpenAI API Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"<ur key>\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for event in stream:\n",
    "        print(event)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3ae72",
   "metadata": {},
   "source": [
    "### Generator 'yiled' and Generator expression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
