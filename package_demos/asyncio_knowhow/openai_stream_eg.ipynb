{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbafd4f9",
   "metadata": {},
   "source": [
    "## Asyncio\n",
    "\n",
    "**You can only use await inside of functions created with async def**\n",
    "\n",
    "Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95e784",
   "metadata": {},
   "source": [
    "### Difference between Threads and Coroutines\n",
    "Threads and coroutines in Python are both mechanisms for achieving concurrency, but they are fundamentally different in how they operate and are managed.\n",
    "\n",
    "**Threads** are managed by the operating system. Each thread represents a separate flow of execution, with its own stack and memory space. Threads can run in parallel (especially on multi-core CPUs), and the OS schedules when each thread runs. This makes threads suitable for tasks that require true parallelism, such as CPU-bound operations. However, threads come with overhead due to context switching and require careful synchronization when accessing shared resources. who understands this better than an embedded systems engineer.\n",
    "\n",
    "**Coroutines** are a programming construct managed at the language level (not by the OS). In Python, coroutines are special functions that can pause (yield) their execution and resume later, allowing other coroutines to run in the meantime. This is known as cooperative multitasking: only one coroutine runs at a time within a **single thread**, and they explicitly yield control to each other. Coroutines are lightweight, have much lower overhead than threads, and are especially well-suited for I/O-bound scenarios. Coroutines are used to perform multiple tasks cooperatively within a single thread, allowing more efficient use of resources and easier management of concurrent tasks. \n",
    "\n",
    "| Feature         | Threads                                 | Coroutines                                   |\n",
    "|-----------------|-----------------------------------------|-----------------------------------------------|\n",
    "| **Management**  | OS-level                               | Language/runtime-level                        |\n",
    "| **Parallelism** | Can run in parallel (multi-core CPUs)  | Run cooperatively within a single thread      |\n",
    "| **Overhead**    | High (context switching, memory)       | Low (lightweight, minimal context switching)  |\n",
    "| **Use case**    | CPU-bound, true parallelism            | I/O-bound, high concurrency, async tasks      |\n",
    "| **Control**     | Preemptive (OS decides when to switch) | Cooperative (programmer decides when to yield)|\n",
    "\n",
    "\n",
    "In summary, threads are for parallel execution managed by the OS, while coroutines are for cooperative multitasking managed by Python itself. Coroutines are not a replacement for threads, but offer a more efficient and simpler way to handle many concurrent I/O-bound tasks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cc47b",
   "metadata": {},
   "source": [
    "### Concurrency and Parallelism: Key Concepts\n",
    "\n",
    "What Is Concurrency?\n",
    "Concurrency in computing means the ability of a system to handle multiple tasks or operations at (almost) the same time. These tasks may not actually run simultaneously—instead, they are managed so that progress is made on each over a period of time. The operating system or language runtime switches between them efficiently, giving the illusion that they are happening together.\n",
    "    * Concurrency is about dealing with many things at once, not necessarily doing them at the same instant.\n",
    "    * Examples: Handling multiple network requests in a web server, or responsive user interfaces where several activities seem active together.\n",
    "\n",
    "What Is Parallelism?\n",
    "Parallelism refers to actually performing multiple computations or tasks at the same time. This is only possible on hardware with multiple processors or cores.\n",
    "    * Parallelism is about doing many things simultaneously, leveraging the hardware’s ability to run code on different cores or machines.\n",
    "    * Examples: Processing parts of a large image on different CPU cores, or running multiple calculations at the same time in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323954c",
   "metadata": {},
   "source": [
    "How Do Threads and Coroutines Relate?\n",
    "Threads\n",
    "\t•\tThreads are a way to achieve both concurrency and, on multi-core systems, parallelism.\n",
    "\t•\tIn Python, threads allow multiple parts of a program to appear to run independently. Operating systems can truly run threads in parallel on different CPU cores.\n",
    "\t•\tHowever, due to Python’s Global Interpreter Lock (GIL), true parallelism is limited for CPU-bound tasks in standard CPython. For I/O-bound programs (like waiting for files or network), threads are very effective.\n",
    "\n",
    "| Feature                | Threads                                   |\n",
    "|------------------------|-------------------------------------------|\n",
    "| Managed by             | Operating system                          |\n",
    "| Can provide parallelism| Yes, on multi-core CPUs                   |\n",
    "| Use-case               | Both concurrency and parallelism          |\n",
    "| Typical tasks          | I/O- or CPU-bound                         |\n",
    "\n",
    "\n",
    "Coroutines\n",
    "\t•\tCoroutines are a lightweight form of concurrency, managed by Python itself (not the OS).\n",
    "\t•\tThey enable the program to switch between tasks at certain points (like when one is waiting for data), but only one coroutine runs at a time within a single thread.\n",
    "\t•\tCoroutines do not give true parallelism (all run on the same thread) but can provide very high concurrency for I/O-bound programs, such as thousands of simultaneous web requests.\n",
    "\n",
    "| Feature                | Coroutines                                |\n",
    "|------------------------|-------------------------------------------|\n",
    "| Managed by             | Python interpreter (language-level)       |\n",
    "| Can provide parallelism| No (single thread, cooperative switching) |\n",
    "| Use-case               | High concurrency, I/O-bound workloads     |\n",
    "| Typical tasks          | Asynchronous network/server programming   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f18dc",
   "metadata": {},
   "source": [
    "#### Comparing Concurrency and Parallelism in Python\n",
    "\n",
    "| Mechanism    | Concurrency | Parallelism | Where Used       | When to Use                                   |\n",
    "|--------------|-------------|-------------|------------------|-----------------------------------------------|\n",
    "| Threads      | Yes         | Limited*    | OS-level tasks   | I/O-bound (good), CPU-bound (limited by GIL)  |\n",
    "| Coroutines   | Yes         | No          | async/await code | High I/O concurrency, very lightweight        |\n",
    "\n",
    "*Python threads can use multiple cores only in implementations without the GIL, or via multiprocessing modules.\n",
    "Summary\n",
    "\t•\tConcurrency is about managing many tasks efficiently, switching between them.\n",
    "\t•\tParallelism is about running multiple tasks at the same time, on separate processors/cores.\n",
    "\t•\tThreads can give both, but Python’s GIL restricts true CPU-based parallelism (except for I/O tasks).\n",
    "\t•\tCoroutines offer high concurrency by pausing and resuming tasks in the same thread, with minimal resource use, but do not provide actual parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7dfa4",
   "metadata": {},
   "source": [
    "`asyncio` module cannot be used in Python environments that run on WebAssembly platforms. \n",
    "\n",
    "The main reason is that these platforms do not support many of the low-level system features required by `asyncio`, such as system calls and event loop support. \n",
    "\n",
    "Therefore, try running the examples in this guide in a regular terminal environment where all necessary system features are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfae5ed",
   "metadata": {},
   "source": [
    "| Mechanism    | Concurrency | Parallelism | Where Used       | When to Use                                   |\n",
    "|--------------|-------------|-------------|------------------|-----------------------------------------------|\n",
    "| Threads      | Yes         | Yes         | OS-level tasks   | I/O-bound (good), CPU-bound                   |\n",
    "| Coroutines   | Yes         | No          | async/await code | High I/O concurrency, very lightweight        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587a1cb",
   "metadata": {},
   "source": [
    "## Streaming in OpenAI API\n",
    "Streaming makes large language model (LLM) apps feel faster and more interactive. Instead of waiting for the full response to be generated, the API sends it in small parts — token by token — as soon as they’re ready. This improves user experience, reduces memory load, allows early cancellation, and handles long responses more efficiently.\n",
    "\n",
    "In OpenAI's API, this is enabled by setting stream=True in the request. When streaming is enabled, the response is delivered using **Server-Sent Events (SSE)** over HTTP with chunked transfer encoding. [Technically, stream=True enables server-sent events (SSE), so the HTTP response is kept open and data is sent incrementally using chunked transfer encoding]\n",
    "\n",
    "Below, we’ll look at how responses differ with and without streaming, then explore how tools like LangChain and LangGraph build on this concept using Python's async and await."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410d450",
   "metadata": {},
   "source": [
    "### Let's learn a bit about SSE\n",
    "\n",
    "Server-Sent Events (SSE) is a web standard for sending real-time updates from a server to a client over a single HTTP connection. SSE lets servers push data to web clients as soon as it is available, making it useful for live data feeds like chat messages or notifications.\n",
    "\n",
    "When you set the `stream` parameter to `True` in OpenAI’s API, it enables streaming responses. This means the API sends back the model’s output in small pieces as soon as each part is ready, instead of waiting for the whole response. This allows clients to show partial results right away, reducing wait time and making applications more interactive.\n",
    "\n",
    "#### How `stream=True` works and its link to SSE\n",
    "\n",
    "- With `stream=True`, the OpenAI API uses Server-Sent Events (SSE). This keeps the HTTP connection open and sends data in chunks, one after another.\n",
    "- Each chunk is sent as a separate event, often called a \"data-only server-sent event.\"\n",
    "- Every chunk contains a small part (called `delta`) of the generated text.\n",
    "- On the client side, you receive these chunks as they arrive. In Python, you can use an asynchronous iterator (like `async for`) to process each chunk as soon as it is received.\n",
    "\n",
    "SSE is simple, reliable, and works well for streaming data from the server to the client in real time. In the context of OpenAI’s API, it helps build responsive apps that can display or process model outputs as soon as they are generated.\n",
    "\n",
    "To build an API that responds with streaming via Server-Sent Events (SSE), the server keeps an HTTP connection open and pushes data to the client in a special plaintext format. Here’s how SSE works and how you would implement it—especially using FastAPI:\n",
    "The server sets headers:\n",
    "\t•\t`Content-Type: text/event-stream`\n",
    "\t•\t`Cache-Control: no-cache`\n",
    "\t•\t`Connection: keep-alive`\n",
    "\n",
    "It keeps the HTTP response open and writes text-formatted events such as:\n",
    "The server emits new data whenever available, and the client receives each without requesting again.\n",
    "\t•\tNo special protocol needed—just HTTP, designed for one-way push from server to browser\n",
    "\n",
    "A simple example of SSE support in FastAPI through its `StreamingResponse`:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def sse_generator():\n",
    "    counter = 1\n",
    "    while counter < 10:\n",
    "        yield f\"data: Event {counter}\\n\\n\"\n",
    "        counter += 1\n",
    "        await asyncio.sleep(2)  # simulate pushing data every 2 seconds\n",
    "\n",
    "# cmd: fastapi dev fastapi_streaming.py\n",
    "@app.get('/sse')\n",
    "async def sse_endpoint():\n",
    "    return StreamingResponse(sse_generator(), media_type='text/event-stream')\n",
    "```\n",
    "Launch postman send the request and observe the events being received in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0db9ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00df7f",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **without** Streaming\n",
    "This code invokes the OpenAI API using the synchronous Python client and does not use streaming. The entire model output is returned in one HTTP response, and the call is blocking: program execution pauses until the response arrives. This approach is straightforward and suitable for scripts or applications where concurrent handling of multiple requests or real-time partial output is not required.\n",
    "* “Synchronous” means the program waits for the API call to complete before moving forward.\n",
    "* “Blocking” means no other statements after the API call are executed until the full response is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ce177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(response): <class 'openai.types.chat.chat_completion.ChatCompletion'> \n",
      "\n",
      "Sure! Double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"typeof(response):\", type(response), \"\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5304d",
   "metadata": {},
   "source": [
    "### Synchronous OpenAI API Call **with** Streaming\n",
    "When `stream=True` is set, the OpenAI API keeps the HTTP connection open and transmits the model’s output incrementally as it is generated. This allows your program to receive and process each chunk of the response in real time, rather than waiting for the complete output. Internally, the API sends these chunks using server-sent events and chunked transfer encoding, enabling your application to start displaying or utilizing the model’s reply as soon as the first data arrives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb68854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(stream): <class 'openai.Stream'>  Is Generator:  False  Is Iterator:  True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This script demonstrates how to use the OpenAI Python client to create a response with stream=True\n",
    "\n",
    "from openai import OpenAI\n",
    "from types import GeneratorType\n",
    "from collections.abc import Iterator\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath' ten times slowly.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "print(\"typeof(stream):\", type(stream), \" Is Generator: \", isinstance(stream, GeneratorType), \" Is Iterator: \", isinstance(stream, Iterator), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767e99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)\n",
    "print(stream.__next__().choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d76343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here it is:\n",
      "\n",
      "Double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath, double bubble bath.None"
     ]
    }
   ],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac16880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for event in stream:\n",
    "\n",
    "    print(\"****\" * 20)\n",
    "    print(json.dumps(event.model_dump(), indent=2))\n",
    "    print(\"@@@@\" * 20)\n",
    "    print(event)\n",
    "    print(\"----\" * 20)\n",
    "\n",
    "    print(event.choices[0].delta.content, end=\"\", flush=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624115ac",
   "metadata": {},
   "source": [
    "### Asynchronous OpenAI API Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"<ur key>\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for event in stream:\n",
    "        print(event)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb4bc1",
   "metadata": {},
   "source": [
    "#### AsyncOpenAI response client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7045b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    response = await client.responses.create(\n",
    "        model=\"gpt-4o\", input=\"Explain disestablishmentarianism to a smart five year old.\"\n",
    "    )\n",
    "    print(response.output_text)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3ae72",
   "metadata": {},
   "source": [
    "### Generator 'yield' and Generator expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da351dc",
   "metadata": {},
   "source": [
    "def async\n",
    "await\n",
    "def asyncio.gather\n",
    "async for\n",
    "asyncio.sleep\n",
    "asyncio.run\n",
    "asyncio.create_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3850d2",
   "metadata": {},
   "source": [
    "### Table\n",
    "||Stream=False|Stream=True|\n",
    "|---|---|----|\n",
    "|Synchronous (blocking call)|Entire response returned at once.|Response streamed in chunks (blocking loop).|\n",
    "|Asynchronous (non-blocking call)|Entire response available after awaiting.|Response streamed in chunks (most efficient and responsive approach).|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f988c",
   "metadata": {},
   "source": [
    "### An example for Asynchronous, non-blocking with streaming off\n",
    "\n",
    "Yes, you are correct: you can use the `AsyncOpenAI` client without setting `stream=True`. In this case, the operation remains asynchronous—meaning you use `async` and `await` syntax—but the response is delivered all at once after processing is complete, not incrementally streamed in chunks.\n",
    "Key points:\n",
    "* With `AsyncOpenAI`, if you omit `stream=True` or set `stream=False`, the call behaves like a regular async HTTP request:\n",
    "* You `await` the completion, and get the entire response in one object, not as a stream.\n",
    "* This is functionally similar to the synchronous (blocking) call, except your code does not block the event loop; it lets other async tasks run while waiting for the response.\n",
    "* Example usage:\n",
    "```python openai_asyncio_stream_off.py```\n",
    "\n",
    "So, using `AsyncOpenAI` without streaming is valid and useful when you want non-blocking API calls, but do not need real-time chunked output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc47ea",
   "metadata": {},
   "source": [
    "## 1. `async def` in asyncio Python (asynchronous programming)\n",
    "\n",
    "Allow your programs to handle many tasks at once without blocking the main thread. This is particularly useful for I/O-bound operations like network requests, file I/O, database queries, or waiting for external resources that can slow down your application.\n",
    "\n",
    "Coroutines are at the heart of asyncio programming in Python. They allow you to write asynchronous code that can pause and resume execution, enabling efficient handling of I/O-bound tasks without blocking the main thread.\n",
    "\n",
    "Key concepts:\n",
    "1. `async def` defines a coroutine, which is a special kind of function that can be paused and resumed. Calling that special function returns a coroutine object instead of running the code immediately. The way to run the coroutine is by passing it to `await`, `asyncio.run()`, `asyncio.create_task()` or `asyncio.gather()`.\n",
    "\n",
    "The below code results in \"RuntimeWarning: coroutine 'greet' was never awaited\"\n",
    "``` python\n",
    "async def greet():\n",
    "    print(\"Hello\")\n",
    "if __name__ == \"__main__\":\n",
    "    greet()\n",
    "```\n",
    "\n",
    "Use `asyncio.run()` to run the coroutine:\n",
    "```python\n",
    "import asyncio\n",
    "async def greet():\n",
    "    print(\"Hello\")\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(greet())\n",
    "```\n",
    "You can define an async def function without using await inside it. This is valid Python, though uncommon in practice. The function is still a coroutine and must be called as such, but it will execute just like a regular function without pausing for any asynchronous operations.\n",
    "\n",
    "To inspect whether a function is a coroutine function, `inspect.iscoroutinefunction()` can be used. It returns True if the function is defined with `async def`, indicating it is a coroutine function.\n",
    "```python\n",
    "import inspect\n",
    "async def greet():\n",
    "    print(\"Hello\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(inspect.iscoroutinefunction(greet)) # Output: True\n",
    "```\n",
    "\n",
    "Similarly, to check if an object is a coroutine object (the result of calling a coroutine function), use `inspect.iscoroutine()` which returns `True` if the object is a coroutine.\n",
    "\n",
    "```\n",
    "cor_obj = greet()\n",
    "print(inspect.iscoroutine(cor_obj))\n",
    "```\n",
    "Additionally, `inspect.isawaitable()` can be used to check if an object can be awaited (i.e., can be used with `await`), which includes coroutine objects, but also other awaitables like Tasks or Futures.\n",
    "\n",
    "\n",
    "|Check|Use|\n",
    "|---|---|\n",
    "|Check if a function is coroutine|`inspect.iscoroutinefunction()`|\n",
    "|Check if an object is coroutine|`inspect.iscoroutine()`|\n",
    "|Check if an object is awaitable|`inspect.isawaitable()`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607efdc6",
   "metadata": {},
   "source": [
    "## 2. `await` in asynio\n",
    "1. It works only inside `async def` functions.\n",
    "2. `await` is used to pause the current coroutine until the result of an “awaitable” object is ready, such as another coroutine, Tasks or an asynchronous generator.\n",
    "3. It makes your code non-blocking by yielding control back to the event loop until the awaited result is available.\n",
    "\n",
    "In summary,\n",
    "\n",
    "|Concept|What it means|\n",
    "|---|---|\n",
    "|`async def`|Declares a coroutine function (returns a coroutine object)|\n",
    "|`await`|Pauses current coroutine until awaitable is finished|\n",
    "\n",
    "An **event loop** in Python’s `asyncio` is the central component that manages and schedules multiple asynchronous tasks (coroutines) to run efficiently in a single thread. It repeatedly cycles through a queue of tasks, running those that are ready, and pausing those waiting for operations like I/O, timers, or other events. This allows your program to run multiple operations concurrently without blocking. Thus, Python’s `asyncio` event loop efficiently switches between tasks that can make progress, pausing on `await`s until awaited results are ready, achieving concurrency without multi-threading.\n",
    "\n",
    "``` python\n",
    "import asyncio\n",
    "\n",
    "async def say_after(delay, greeting_msg):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(greeting_msg)\n",
    "\n",
    "async def main():\n",
    "    print(\"Start\")\n",
    "    # Schedule multiple coroutines concurrently\n",
    "    await asyncio.gather(\n",
    "        say_after(1, \"Hello\"),\n",
    "        say_after(2, \"World\")\n",
    "    )\n",
    "    print(\"End\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d0b20",
   "metadata": {},
   "source": [
    "## 3. `asyncio.run()` in asyncio\n",
    "1. This function is a simple way to start an asyncio program. It runs the main entry-point coroutine and manages the event loop.\n",
    "2. It creates a new event loop, runs the specified coroutine until it completes, and then closes the loop.\n",
    "3. You can checkout asyncio.new_event_loop() and asyncio.set_event_loop() for more advanced use cases (manual loop management), but `asyncio.run()` is the recommended way to run a top-level coroutine in most cases.\n",
    "\n",
    "Note: You can call `asyncio.run()` multiple times in the same Python program — each call creates a fresh event loop and closes it after use. However, it’s generally recommended to call it only once per program execution, typically at the entry point of your asynchronous application (e.g., `asyncio.run(main())`). Repeated use is valid but discouraged in complex or performance-critical applications due to inefficiency and potential side effects. For running multiple coroutines, manage them inside a single async function using tools like `asyncio.gather()` or `asyncio.create_task()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d5885",
   "metadata": {},
   "source": [
    "## 4. `asyncio.gather()` in asyncio\n",
    "1. It takes multiple awaitable objects (coroutines, Tasks, or Futures) and runs them in parallel, returning their results as a list.\n",
    "2. The `asyncio.gather()` function itself only returns an awaitable that schedules the provided coroutines, but it does not create or run the event loop by itself.\n",
    "3. To actually execute the coroutines passed to `asyncio.gather()`, there must be an event loop already running.\n",
    "4. Calling `asyncio.gather()` without awaiting its result inside an active event loop will not execute the coroutines.\n",
    "5. So, you need to be inside a coroutine running on an event loop (typically started with `asyncio.run()` or manually) to `await asyncio.gather()` and get it to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b018d1",
   "metadata": {},
   "source": [
    "## 5. `asyncio.create_task()` in asyncio\n",
    "1. `asyncio.create_task()` takes a coroutine and wraps it in a Task object, scheduling it to run concurrently on the event loop right away. In contrast, simply calling an `async def` function returns a coroutine object, but does not start its execution until it is awaited or scheduled as a Task.\n",
    "2. `asyncio.create_task(coroutine_object)` submits the coroutine to the event loop right away, so it runs concurrently alongside other tasks and coroutines.\n",
    "3. You can later `await` the Task to get its result or just let it run independently if you don’t need the result right away.\n",
    "\n",
    "``` python\n",
    "import asyncio\n",
    "\n",
    "async def say_after(delay, message):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(message)\n",
    "\n",
    "async def main():\n",
    "    # Create tasks to run concurrently\n",
    "    task1 = asyncio.create_task(say_after(2, \"Hello\"))\n",
    "    task2 = asyncio.create_task(say_after(1, \"World\"))\n",
    "\n",
    "    print(\"Tasks started\")\n",
    "    # Do other things here while the tasks run...\n",
    "\n",
    "    # Await the tasks to get their result (wait until they finish)\n",
    "    await task1\n",
    "    await task2\n",
    "    print(\"Both tasks finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "Try commenting out await task1 or task2 in the above code to see how it affects the output. If you do not await a task, it will still run concurrently, but you won't wait for its completion before moving on to the next line of code.\n",
    "Now, add `await asyncio.sleep(10)` after print(\"Both tasks finished\") to see how the program behaves when you wait for a while after the tasks are done. This will keep the event loop running, allowing any remaining tasks to complete before the program exits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b98a5a",
   "metadata": {},
   "source": [
    "## 6. `asynio.sleep()` in asyncio\n",
    "This is an async version of `time.sleep()`. It lets a coroutine pause without blocking the whole program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae4655",
   "metadata": {},
   "source": [
    "## 7. `async for` in asyncio\n",
    "1. `async for` is used to iterate over asynchronous iterators, such as those that fetch data in chunks. Example is using AsyncOpenAI client to stream data from OpenAI API.\n",
    "2. It allows you to process items as they become available, without blocking the event loop.\n",
    "3. Asynchronous iterators are objects that implement the `__aiter__()` and `__anext__()` methods like __iter__() and __next__() in iterators. \n",
    "4. __aiter__() returns an asynchronous iterator object, typically self.\n",
    "5. __anext__() must return an awaitable object. `async for` resolves the awaitables returned by an asynchronous iterator's __anext__() method until it raises a StopAsyncIteratoion exception.\n",
    "\n",
    "```python\n",
    "class Counter:\n",
    "    def __init__(self, low, high):\n",
    "        self.current = low\n",
    "        self.high = high\n",
    "\n",
    "    def __aiter__(self):\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        if self.current > self.high:\n",
    "            raise StopAsyncIteration\n",
    "        await asyncio.sleep(1)  # Simulate an asynchronous operation\n",
    "        self.current += 1\n",
    "        return self.current - 1\n",
    "\n",
    "async def main():\n",
    "    async for number in Counter(1, 5):\n",
    "        print(number)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59d226",
   "metadata": {},
   "source": [
    "`AsyncOpenAI` and setting `stream=True` are two different concepts.\n",
    "\t•\t`AsyncOpenAI` is the asynchronous version of the OpenAI Python client. It provides async methods to call OpenAI APIs, allowing you to use `async/await` syntax, run many requests concurrently, and generally leverage Python’s `asyncio` concurrency functionality. It’s about how you interact with the OpenAI API (async client vs. sync client).\n",
    "\t•\t`stream=True` is an option you pass in a request to enable streaming responses from the API. Normally, the API sends you the whole response after it’s fully generated. When you use `stream=True`, you receive the response incrementally as it’s being generated, typically as an asynchronous iterator or async generator that yields events/chunks of the output. Streaming reduces latency and lets you process partial results early.\n",
    "\n",
    "    You can use `AsyncOpenAI` without `stream=True` to get full responses asynchronously, or with `stream=True` to get incremental streaming responses asynchronously.\n",
    "So they serve different roles:\n",
    "\t•\t`AsyncOpenAI` is about how you call the API (async client).\n",
    "\t•\t`stream=True` is about how the API returns the data (streaming mode)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
